{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Defining New autograd Functions\n",
    "----------------------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation computes the forward pass using operations on PyTorch\n",
    "Variables, and uses PyTorch autograd to compute gradients.\n",
    "\n",
    "In this implementation we implement our own custom autograd function to perform\n",
    "the ReLU function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31413648.0\n",
      "1 25686550.0\n",
      "2 24327268.0\n",
      "3 23221008.0\n",
      "4 20491104.0\n",
      "5 15970276.0\n",
      "6 11062328.0\n",
      "7 7004372.0\n",
      "8 4294336.5\n",
      "9 2670113.5\n",
      "10 1750279.0\n",
      "11 1225514.25\n",
      "12 915114.25\n",
      "13 719471.25\n",
      "14 587329.1875\n",
      "15 491794.625\n",
      "16 418853.125\n",
      "17 360896.21875\n",
      "18 313515.6875\n",
      "19 274034.25\n",
      "20 240663.65625\n",
      "21 212193.875\n",
      "22 187689.0625\n",
      "23 166496.75\n",
      "24 148108.40625\n",
      "25 132061.546875\n",
      "26 118025.4765625\n",
      "27 105703.984375\n",
      "28 94864.203125\n",
      "29 85294.8671875\n",
      "30 76827.46875\n",
      "31 69323.7265625\n",
      "32 62650.16015625\n",
      "33 56706.90625\n",
      "34 51401.4921875\n",
      "35 46652.5390625\n",
      "36 42396.54296875\n",
      "37 38579.24609375\n",
      "38 35146.515625\n",
      "39 32056.978515625\n",
      "40 29275.861328125\n",
      "41 26765.068359375\n",
      "42 24495.830078125\n",
      "43 22441.58203125\n",
      "44 20578.86328125\n",
      "45 18889.59375\n",
      "46 17354.28125\n",
      "47 15958.3818359375\n",
      "48 14688.8037109375\n",
      "49 13531.3935546875\n",
      "50 12475.6728515625\n",
      "51 11511.451171875\n",
      "52 10629.66796875\n",
      "53 9823.748046875\n",
      "54 9085.466796875\n",
      "55 8408.5634765625\n",
      "56 7787.35107421875\n",
      "57 7217.30810546875\n",
      "58 6693.40576171875\n",
      "59 6211.77392578125\n",
      "60 5768.68798828125\n",
      "61 5360.4482421875\n",
      "62 4983.82421875\n",
      "63 4636.427734375\n",
      "64 4315.79296875\n",
      "65 4019.47509765625\n",
      "66 3745.46484375\n",
      "67 3491.85107421875\n",
      "68 3257.1494140625\n",
      "69 3039.7958984375\n",
      "70 2838.185546875\n",
      "71 2651.306640625\n",
      "72 2477.7705078125\n",
      "73 2316.604736328125\n",
      "74 2166.93212890625\n",
      "75 2027.7344970703125\n",
      "76 1898.257080078125\n",
      "77 1777.78466796875\n",
      "78 1665.6231689453125\n",
      "79 1561.149169921875\n",
      "80 1463.804931640625\n",
      "81 1372.958251953125\n",
      "82 1288.23583984375\n",
      "83 1209.176513671875\n",
      "84 1135.3533935546875\n",
      "85 1066.434326171875\n",
      "86 1001.9901123046875\n",
      "87 941.7871704101562\n",
      "88 885.4285278320312\n",
      "89 832.6929931640625\n",
      "90 783.3323974609375\n",
      "91 737.1099853515625\n",
      "92 693.7913818359375\n",
      "93 653.2020263671875\n",
      "94 615.137451171875\n",
      "95 579.4546508789062\n",
      "96 546.0029296875\n",
      "97 514.572265625\n",
      "98 485.0667724609375\n",
      "99 457.35888671875\n",
      "100 431.3580627441406\n",
      "101 406.91375732421875\n",
      "102 383.93487548828125\n",
      "103 362.32989501953125\n",
      "104 342.0118408203125\n",
      "105 322.91448974609375\n",
      "106 304.930908203125\n",
      "107 287.9956359863281\n",
      "108 272.0517883300781\n",
      "109 257.03509521484375\n",
      "110 242.8969268798828\n",
      "111 229.5711212158203\n",
      "112 217.01132202148438\n",
      "113 205.17376708984375\n",
      "114 194.01446533203125\n",
      "115 183.49209594726562\n",
      "116 173.56680297851562\n",
      "117 164.1991424560547\n",
      "118 155.35739135742188\n",
      "119 147.0146026611328\n",
      "120 139.140869140625\n",
      "121 131.7021026611328\n",
      "122 124.68048095703125\n",
      "123 118.04700469970703\n",
      "124 111.78263092041016\n",
      "125 105.86105346679688\n",
      "126 100.26680755615234\n",
      "127 94.98068237304688\n",
      "128 89.98182678222656\n",
      "129 85.2554931640625\n",
      "130 80.78716278076172\n",
      "131 76.55912780761719\n",
      "132 72.5589370727539\n",
      "133 68.7762451171875\n",
      "134 65.1982650756836\n",
      "135 61.812042236328125\n",
      "136 58.60643005371094\n",
      "137 55.572235107421875\n",
      "138 52.70179748535156\n",
      "139 49.98454284667969\n",
      "140 47.41014862060547\n",
      "141 44.97156524658203\n",
      "142 42.66108703613281\n",
      "143 40.474395751953125\n",
      "144 38.40276336669922\n",
      "145 36.439842224121094\n",
      "146 34.57887268066406\n",
      "147 32.81557846069336\n",
      "148 31.146007537841797\n",
      "149 29.56329345703125\n",
      "150 28.06237030029297\n",
      "151 26.64019775390625\n",
      "152 25.291744232177734\n",
      "153 24.01300048828125\n",
      "154 22.80006217956543\n",
      "155 21.64960479736328\n",
      "156 20.558977127075195\n",
      "157 19.524194717407227\n",
      "158 18.54334831237793\n",
      "159 17.611778259277344\n",
      "160 16.728891372680664\n",
      "161 15.890393257141113\n",
      "162 15.095026016235352\n",
      "163 14.340803146362305\n",
      "164 13.625177383422852\n",
      "165 12.945828437805176\n",
      "166 12.300492286682129\n",
      "167 11.688133239746094\n",
      "168 11.10722541809082\n",
      "169 10.555242538452148\n",
      "170 10.031488418579102\n",
      "171 9.53365707397461\n",
      "172 9.061257362365723\n",
      "173 8.613122940063477\n",
      "174 8.187301635742188\n",
      "175 7.782586097717285\n",
      "176 7.398398399353027\n",
      "177 7.033344268798828\n",
      "178 6.687491416931152\n",
      "179 6.357970237731934\n",
      "180 6.045305252075195\n",
      "181 5.748157024383545\n",
      "182 5.4659318923950195\n",
      "183 5.197953224182129\n",
      "184 4.9430832862854\n",
      "185 4.700872421264648\n",
      "186 4.470939636230469\n",
      "187 4.252179145812988\n",
      "188 4.044448375701904\n",
      "189 3.8470239639282227\n",
      "190 3.659226417541504\n",
      "191 3.480879068374634\n",
      "192 3.311232089996338\n",
      "193 3.15000057220459\n",
      "194 2.99680233001709\n",
      "195 2.851170539855957\n",
      "196 2.71254825592041\n",
      "197 2.580789566040039\n",
      "198 2.455599546432495\n",
      "199 2.336575984954834\n",
      "200 2.2233734130859375\n",
      "201 2.115696907043457\n",
      "202 2.0133275985717773\n",
      "203 1.9159785509109497\n",
      "204 1.8234457969665527\n",
      "205 1.735449194908142\n",
      "206 1.6515541076660156\n",
      "207 1.5718786716461182\n",
      "208 1.4960836172103882\n",
      "209 1.4241482019424438\n",
      "210 1.355525016784668\n",
      "211 1.2902805805206299\n",
      "212 1.2281607389450073\n",
      "213 1.1690053939819336\n",
      "214 1.112915277481079\n",
      "215 1.05954909324646\n",
      "216 1.0087430477142334\n",
      "217 0.9603419303894043\n",
      "218 0.9143218994140625\n",
      "219 0.8705447316169739\n",
      "220 0.8288515210151672\n",
      "221 0.7891924977302551\n",
      "222 0.7515241503715515\n",
      "223 0.7155319452285767\n",
      "224 0.6814299821853638\n",
      "225 0.6488939523696899\n",
      "226 0.6179627180099487\n",
      "227 0.5884680151939392\n",
      "228 0.5604593753814697\n",
      "229 0.5337854623794556\n",
      "230 0.5083610415458679\n",
      "231 0.48411861062049866\n",
      "232 0.4610656201839447\n",
      "233 0.4392189681529999\n",
      "234 0.41832271218299866\n",
      "235 0.3984528183937073\n",
      "236 0.3795627951622009\n",
      "237 0.36150771379470825\n",
      "238 0.34436550736427307\n",
      "239 0.3280671238899231\n",
      "240 0.3125571608543396\n",
      "241 0.29779475927352905\n",
      "242 0.28366535902023315\n",
      "243 0.2702719569206238\n",
      "244 0.2575069069862366\n",
      "245 0.24530862271785736\n",
      "246 0.2337515503168106\n",
      "247 0.22273576259613037\n",
      "248 0.21217551827430725\n",
      "249 0.20216408371925354\n",
      "250 0.19269996881484985\n",
      "251 0.18357780575752258\n",
      "252 0.17495527863502502\n",
      "253 0.16670995950698853\n",
      "254 0.15888138115406036\n",
      "255 0.15138843655586243\n",
      "256 0.14428099989891052\n",
      "257 0.13748247921466827\n",
      "258 0.13104282319545746\n",
      "259 0.12489504367113113\n",
      "260 0.11906525492668152\n",
      "261 0.113458551466465\n",
      "262 0.1081620454788208\n",
      "263 0.10310567915439606\n",
      "264 0.09825927019119263\n",
      "265 0.09366262704133987\n",
      "266 0.08927126228809357\n",
      "267 0.08511055260896683\n",
      "268 0.08113209903240204\n",
      "269 0.07735773175954819\n",
      "270 0.07374940812587738\n",
      "271 0.07029973715543747\n",
      "272 0.06702360510826111\n",
      "273 0.06390559673309326\n",
      "274 0.06091272830963135\n",
      "275 0.05808581784367561\n",
      "276 0.055387917906045914\n",
      "277 0.0528210774064064\n",
      "278 0.0503770187497139\n",
      "279 0.048044465482234955\n",
      "280 0.045812979340553284\n",
      "281 0.043688639998435974\n",
      "282 0.0416593924164772\n",
      "283 0.03973989188671112\n",
      "284 0.03789869323372841\n",
      "285 0.036145955324172974\n",
      "286 0.03447885066270828\n",
      "287 0.03288954496383667\n",
      "288 0.0313752144575119\n",
      "289 0.029917502775788307\n",
      "290 0.028542721644043922\n",
      "291 0.027235355228185654\n",
      "292 0.025989003479480743\n",
      "293 0.024781513959169388\n",
      "294 0.023645570501685143\n",
      "295 0.0225689634680748\n",
      "296 0.021534454077482224\n",
      "297 0.02054879069328308\n",
      "298 0.01961434632539749\n",
      "299 0.018725641071796417\n",
      "300 0.017867041751742363\n",
      "301 0.017053525894880295\n",
      "302 0.01627090759575367\n",
      "303 0.015531145967543125\n",
      "304 0.014824683777987957\n",
      "305 0.014156976714730263\n",
      "306 0.013512109406292439\n",
      "307 0.012901636771857738\n",
      "308 0.01232357881963253\n",
      "309 0.011767275631427765\n",
      "310 0.011244024150073528\n",
      "311 0.010743651539087296\n",
      "312 0.010255556553602219\n",
      "313 0.00980530958622694\n",
      "314 0.009359929710626602\n",
      "315 0.00894518755376339\n",
      "316 0.008552447892725468\n",
      "317 0.008174335584044456\n",
      "318 0.007810431998223066\n",
      "319 0.007471195422112942\n",
      "320 0.007144265808165073\n",
      "321 0.0068323202431201935\n",
      "322 0.006531606428325176\n",
      "323 0.006247189827263355\n",
      "324 0.005972667597234249\n",
      "325 0.005714136175811291\n",
      "326 0.005471412558108568\n",
      "327 0.005237485282123089\n",
      "328 0.005014846101403236\n",
      "329 0.0048018526285886765\n",
      "330 0.004594482947140932\n",
      "331 0.0043999385088682175\n",
      "332 0.00421379879117012\n",
      "333 0.0040341708809137344\n",
      "334 0.0038686138577759266\n",
      "335 0.0037042771000415087\n",
      "336 0.003550660563632846\n",
      "337 0.0034078448079526424\n",
      "338 0.003265400417149067\n",
      "339 0.003130906028673053\n",
      "340 0.003002508543431759\n",
      "341 0.002881125546991825\n",
      "342 0.002765440847724676\n",
      "343 0.0026536460500210524\n",
      "344 0.002544966759160161\n",
      "345 0.0024448879994452\n",
      "346 0.0023487263824790716\n",
      "347 0.002256216946989298\n",
      "348 0.0021642358042299747\n",
      "349 0.0020819457713514566\n",
      "350 0.0020004920661449432\n",
      "351 0.0019233089406043291\n",
      "352 0.0018484906759113073\n",
      "353 0.0017798877088353038\n",
      "354 0.001714299782179296\n",
      "355 0.001649048295803368\n",
      "356 0.0015865694731473923\n",
      "357 0.001527657499536872\n",
      "358 0.0014721191255375743\n",
      "359 0.0014187358319759369\n",
      "360 0.0013657859526574612\n",
      "361 0.0013174830237403512\n",
      "362 0.0012697463389486074\n",
      "363 0.0012241480872035027\n",
      "364 0.0011813950259238482\n",
      "365 0.0011410713195800781\n",
      "366 0.0011006358545273542\n",
      "367 0.0010621111141517758\n",
      "368 0.0010271385544911027\n",
      "369 0.0009912513196468353\n",
      "370 0.0009586700471118093\n",
      "371 0.0009254080941900611\n",
      "372 0.0008952064090408385\n",
      "373 0.0008661509491503239\n",
      "374 0.0008359655039384961\n",
      "375 0.0008087960886768997\n",
      "376 0.0007831103866919875\n",
      "377 0.0007581838872283697\n",
      "378 0.0007347483187913895\n",
      "379 0.0007113377214409411\n",
      "380 0.0006896512350067496\n",
      "381 0.0006679934449493885\n",
      "382 0.0006473365938290954\n",
      "383 0.0006282768445089459\n",
      "384 0.0006089532398618758\n",
      "385 0.0005906416336074471\n",
      "386 0.0005726381205022335\n",
      "387 0.0005565238534472883\n",
      "388 0.0005397328059189022\n",
      "389 0.0005235807620920241\n",
      "390 0.0005083612632006407\n",
      "391 0.0004944303072988987\n",
      "392 0.00047990481834858656\n",
      "393 0.00046597467735409737\n",
      "394 0.000452810141723603\n",
      "395 0.0004399341996759176\n",
      "396 0.0004281325964257121\n",
      "397 0.0004156208015047014\n",
      "398 0.0004052267177030444\n",
      "399 0.0003946410142816603\n",
      "400 0.000384270038921386\n",
      "401 0.00037423649337142706\n",
      "402 0.0003636537294369191\n",
      "403 0.000353843963239342\n",
      "404 0.0003455310361459851\n",
      "405 0.00033689767587929964\n",
      "406 0.000327746121911332\n",
      "407 0.0003198876802343875\n",
      "408 0.0003109199751634151\n",
      "409 0.00030332047026604414\n",
      "410 0.000296133104711771\n",
      "411 0.00028934539295732975\n",
      "412 0.00028219446539878845\n",
      "413 0.0002757609763648361\n",
      "414 0.00026836030883714557\n",
      "415 0.00026213680393993855\n",
      "416 0.0002561477886047214\n",
      "417 0.00025000557070598006\n",
      "418 0.0002437452640151605\n",
      "419 0.00023835043248254806\n",
      "420 0.00023277083528228104\n",
      "421 0.00022786876070313156\n",
      "422 0.00022252200869843364\n",
      "423 0.00021712682791985571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 0.00021186517551541328\n",
      "425 0.00020707747898995876\n",
      "426 0.00020306264923419803\n",
      "427 0.0001988906878978014\n",
      "428 0.00019451265688985586\n",
      "429 0.00019030935072805732\n",
      "430 0.00018619338516145945\n",
      "431 0.00018223203369416296\n",
      "432 0.00017831931472755969\n",
      "433 0.00017484009731560946\n",
      "434 0.00017141674470622092\n",
      "435 0.00016742442676331848\n",
      "436 0.00016401856555603445\n",
      "437 0.00016054656589403749\n",
      "438 0.00015697741764597595\n",
      "439 0.00015374629583675414\n",
      "440 0.00015092945250216872\n",
      "441 0.00014823541278019547\n",
      "442 0.00014468689914792776\n",
      "443 0.0001423382491338998\n",
      "444 0.00013918636250309646\n",
      "445 0.00013679885887540877\n",
      "446 0.00013423929340206087\n",
      "447 0.00013149289588909596\n",
      "448 0.00012952665565535426\n",
      "449 0.00012710060400422662\n",
      "450 0.00012436023098416626\n",
      "451 0.00012233969755470753\n",
      "452 0.0001198705576825887\n",
      "453 0.00011792506120400503\n",
      "454 0.00011636711133178324\n",
      "455 0.00011442979302955791\n",
      "456 0.00011222963803447783\n",
      "457 0.0001101930538425222\n",
      "458 0.00010782614117488265\n",
      "459 0.00010644999565556645\n",
      "460 0.0001047117548296228\n",
      "461 0.00010288062185281888\n",
      "462 0.0001011368294712156\n",
      "463 9.945121564669535e-05\n",
      "464 9.793406934477389e-05\n",
      "465 9.64364007813856e-05\n",
      "466 9.512891847407445e-05\n",
      "467 9.371456690132618e-05\n",
      "468 9.211456927005202e-05\n",
      "469 9.053228131961077e-05\n",
      "470 8.91444506123662e-05\n",
      "471 8.80642473930493e-05\n",
      "472 8.651746611576527e-05\n",
      "473 8.505818550474942e-05\n",
      "474 8.334193262271583e-05\n",
      "475 8.248705125879496e-05\n",
      "476 8.128117769956589e-05\n",
      "477 7.986029959283769e-05\n",
      "478 7.900966011220589e-05\n",
      "479 7.764590554870665e-05\n",
      "480 7.680199632886797e-05\n",
      "481 7.577633368782699e-05\n",
      "482 7.472189463442191e-05\n",
      "483 7.380971510428935e-05\n",
      "484 7.251342321978882e-05\n",
      "485 7.135259511414915e-05\n",
      "486 7.032428402453661e-05\n",
      "487 6.94656846462749e-05\n",
      "488 6.82351746945642e-05\n",
      "489 6.737629882991314e-05\n",
      "490 6.653390300925821e-05\n",
      "491 6.550790567416698e-05\n",
      "492 6.459049473050982e-05\n",
      "493 6.391745409928262e-05\n",
      "494 6.298645166680217e-05\n",
      "495 6.223328819032758e-05\n",
      "496 6.132712587714195e-05\n",
      "497 6.044729161658324e-05\n",
      "498 5.988638076814823e-05\n",
      "499 5.949898331891745e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement out own custom autograd Functions by subclassing torch.autograd.Function and implementing the forward\n",
    "    and backward passes which operate on Tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return a Tensor containing the output. \n",
    "        ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output, \n",
    "        and we need to compute the gradient of the loss with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "    \n",
    "\n",
    "    \n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights and set gradients compute\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "    \n",
    "    # Forward pass: computed predicted y using operations. We compute ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 w.r.t loss\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    with torch.no_grad():\n",
    "        w1 -=learning_rate * w1.grad\n",
    "        w2 -=learning_rate * w2.grad\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
