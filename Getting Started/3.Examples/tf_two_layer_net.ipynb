{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TensorFlow: Static Graphs\n",
    "-------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation uses basic TensorFlow operations to set up a computational\n",
    "graph, then executes the graph many times to actually train the network.\n",
    "\n",
    "One of the main differences between TensorFlow and PyTorch is that TensorFlow\n",
    "uses static computational graphs while PyTorch uses dynamic computational\n",
    "graphs.\n",
    "\n",
    "In TensorFlow we first set up the computational graph, then execute the same\n",
    "graph many times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 36774172.0\n",
      "1 39029410.0\n",
      "2 47016332.0\n",
      "3 49462404.0\n",
      "4 38736780.0\n",
      "5 20464478.0\n",
      "6 8282062.5\n",
      "7 3392531.0\n",
      "8 1814877.0\n",
      "9 1244795.1\n",
      "10 969695.5\n",
      "11 794617.5\n",
      "12 665141.0\n",
      "13 563195.6\n",
      "14 480492.03\n",
      "15 412444.56\n",
      "16 355949.78\n",
      "17 308658.25\n",
      "18 268781.9\n",
      "19 235008.31\n",
      "20 206199.83\n",
      "21 181530.31\n",
      "22 160297.56\n",
      "23 141962.05\n",
      "24 126079.57\n",
      "25 112270.58\n",
      "26 100202.39\n",
      "27 89624.04\n",
      "28 80322.875\n",
      "29 72129.94\n",
      "30 64864.1\n",
      "31 58425.246\n",
      "32 52713.766\n",
      "33 47637.016\n",
      "34 43106.62\n",
      "35 39058.215\n",
      "36 35436.05\n",
      "37 32187.445\n",
      "38 29269.957\n",
      "39 26656.014\n",
      "40 24301.195\n",
      "41 22176.883\n",
      "42 20258.234\n",
      "43 18524.008\n",
      "44 16952.121\n",
      "45 15527.13\n",
      "46 14233.115\n",
      "47 13057.084\n",
      "48 11988.27\n",
      "49 11014.54\n",
      "50 10126.842\n",
      "51 9316.969\n",
      "52 8577.85\n",
      "53 7902.2495\n",
      "54 7284.559\n",
      "55 6719.069\n",
      "56 6201.0557\n",
      "57 5725.954\n",
      "58 5290.3086\n",
      "59 4890.1367\n",
      "60 4522.7\n",
      "61 4184.9697\n",
      "62 3874.0835\n",
      "63 3587.981\n",
      "64 3324.74\n",
      "65 3082.044\n",
      "66 2858.2432\n",
      "67 2651.858\n",
      "68 2461.3345\n",
      "69 2285.47\n",
      "70 2122.8684\n",
      "71 1972.55\n",
      "72 1833.5527\n",
      "73 1705.0416\n",
      "74 1586.0264\n",
      "75 1475.7561\n",
      "76 1373.6702\n",
      "77 1279.1185\n",
      "78 1191.3738\n",
      "79 1109.9529\n",
      "80 1034.4231\n",
      "81 964.3248\n",
      "82 899.2477\n",
      "83 838.766\n",
      "84 782.56274\n",
      "85 730.338\n",
      "86 681.76843\n",
      "87 636.599\n",
      "88 594.55945\n",
      "89 555.4612\n",
      "90 519.06494\n",
      "91 485.13977\n",
      "92 453.53226\n",
      "93 424.08862\n",
      "94 396.66257\n",
      "95 371.06882\n",
      "96 347.2097\n",
      "97 324.95068\n",
      "98 304.1911\n",
      "99 284.80536\n",
      "100 266.70438\n",
      "101 249.80392\n",
      "102 234.02296\n",
      "103 219.27873\n",
      "104 205.49876\n",
      "105 192.62027\n",
      "106 180.59135\n",
      "107 169.33328\n",
      "108 158.80298\n",
      "109 148.95465\n",
      "110 139.74371\n",
      "111 131.1186\n",
      "112 123.04555\n",
      "113 115.48825\n",
      "114 108.41807\n",
      "115 101.79152\n",
      "116 95.58357\n",
      "117 89.76506\n",
      "118 84.314415\n",
      "119 79.20268\n",
      "120 74.41219\n",
      "121 69.92288\n",
      "122 65.71355\n",
      "123 61.76248\n",
      "124 58.056084\n",
      "125 54.581833\n",
      "126 51.319954\n",
      "127 48.259148\n",
      "128 45.38591\n",
      "129 42.690796\n",
      "130 40.15788\n",
      "131 37.779625\n",
      "132 35.54606\n",
      "133 33.44947\n",
      "134 31.478832\n",
      "135 29.628548\n",
      "136 27.890326\n",
      "137 26.254438\n",
      "138 24.718067\n",
      "139 23.273624\n",
      "140 21.916597\n",
      "141 20.639019\n",
      "142 19.438522\n",
      "143 18.310162\n",
      "144 17.248116\n",
      "145 16.248854\n",
      "146 15.309038\n",
      "147 14.425035\n",
      "148 13.593413\n",
      "149 12.81069\n",
      "150 12.0740185\n",
      "151 11.380142\n",
      "152 10.727656\n",
      "153 10.113003\n",
      "154 9.533789\n",
      "155 8.989385\n",
      "156 8.476772\n",
      "157 7.993216\n",
      "158 7.538101\n",
      "159 7.1094294\n",
      "160 6.7055073\n",
      "161 6.3246746\n",
      "162 5.9661846\n",
      "163 5.6283827\n",
      "164 5.3098\n",
      "165 5.0098934\n",
      "166 4.7272787\n",
      "167 4.4606905\n",
      "168 4.209216\n",
      "169 3.972441\n",
      "170 3.749105\n",
      "171 3.5388637\n",
      "172 3.340335\n",
      "173 3.153267\n",
      "174 2.976564\n",
      "175 2.8100498\n",
      "176 2.653101\n",
      "177 2.5048776\n",
      "178 2.365056\n",
      "179 2.2330608\n",
      "180 2.108831\n",
      "181 1.9913585\n",
      "182 1.8808298\n",
      "183 1.7762766\n",
      "184 1.677496\n",
      "185 1.5844971\n",
      "186 1.4968914\n",
      "187 1.413877\n",
      "188 1.3356018\n",
      "189 1.261784\n",
      "190 1.1920395\n",
      "191 1.1263403\n",
      "192 1.0642129\n",
      "193 1.0056556\n",
      "194 0.9500905\n",
      "195 0.89785385\n",
      "196 0.84846747\n",
      "197 0.80179167\n",
      "198 0.75769544\n",
      "199 0.7161722\n",
      "200 0.6769356\n",
      "201 0.63981396\n",
      "202 0.60481095\n",
      "203 0.5716438\n",
      "204 0.54036725\n",
      "205 0.51082486\n",
      "206 0.482822\n",
      "207 0.45645875\n",
      "208 0.43158233\n",
      "209 0.40809268\n",
      "210 0.3858599\n",
      "211 0.3648017\n",
      "212 0.34500724\n",
      "213 0.3262084\n",
      "214 0.30849102\n",
      "215 0.29177672\n",
      "216 0.27585086\n",
      "217 0.26093772\n",
      "218 0.24675691\n",
      "219 0.23336308\n",
      "220 0.22072364\n",
      "221 0.2087364\n",
      "222 0.19746563\n",
      "223 0.18678531\n",
      "224 0.17668499\n",
      "225 0.16713367\n",
      "226 0.15811862\n",
      "227 0.14956757\n",
      "228 0.14151609\n",
      "229 0.1338579\n",
      "230 0.1266574\n",
      "231 0.119835995\n",
      "232 0.1133745\n",
      "233 0.10727133\n",
      "234 0.10148462\n",
      "235 0.09601438\n",
      "236 0.090850666\n",
      "237 0.08596715\n",
      "238 0.08136192\n",
      "239 0.07700865\n",
      "240 0.07288448\n",
      "241 0.06898896\n",
      "242 0.06526616\n",
      "243 0.06178729\n",
      "244 0.05846973\n",
      "245 0.055350915\n",
      "246 0.052375965\n",
      "247 0.04958793\n",
      "248 0.046948124\n",
      "249 0.04442808\n",
      "250 0.04206655\n",
      "251 0.039813578\n",
      "252 0.037692893\n",
      "253 0.035681933\n",
      "254 0.033789936\n",
      "255 0.031981442\n",
      "256 0.030300152\n",
      "257 0.028694356\n",
      "258 0.02717698\n",
      "259 0.025723638\n",
      "260 0.024360364\n",
      "261 0.02308385\n",
      "262 0.021856442\n",
      "263 0.020707276\n",
      "264 0.019617476\n",
      "265 0.018583445\n",
      "266 0.01759331\n",
      "267 0.016673364\n",
      "268 0.015808031\n",
      "269 0.014970586\n",
      "270 0.014181726\n",
      "271 0.013446158\n",
      "272 0.012748805\n",
      "273 0.012086078\n",
      "274 0.01145719\n",
      "275 0.010876106\n",
      "276 0.010303354\n",
      "277 0.009769829\n",
      "278 0.00926169\n",
      "279 0.008792657\n",
      "280 0.008336522\n",
      "281 0.007904412\n",
      "282 0.0075001256\n",
      "283 0.0071220044\n",
      "284 0.0067600785\n",
      "285 0.0064205034\n",
      "286 0.00609609\n",
      "287 0.0057845176\n",
      "288 0.005484903\n",
      "289 0.0052117407\n",
      "290 0.004946605\n",
      "291 0.004706765\n",
      "292 0.0044677174\n",
      "293 0.0042495644\n",
      "294 0.004040351\n",
      "295 0.0038399077\n",
      "296 0.003648927\n",
      "297 0.003475641\n",
      "298 0.0033126143\n",
      "299 0.003150201\n",
      "300 0.0029937238\n",
      "301 0.0028528492\n",
      "302 0.002721467\n",
      "303 0.0025942186\n",
      "304 0.0024695126\n",
      "305 0.002354149\n",
      "306 0.002245141\n",
      "307 0.0021430019\n",
      "308 0.0020452396\n",
      "309 0.0019531166\n",
      "310 0.0018637886\n",
      "311 0.0017802007\n",
      "312 0.0017031659\n",
      "313 0.0016285202\n",
      "314 0.0015572375\n",
      "315 0.0014899888\n",
      "316 0.0014256923\n",
      "317 0.0013657443\n",
      "318 0.0013090414\n",
      "319 0.0012522823\n",
      "320 0.0012001913\n",
      "321 0.0011509865\n",
      "322 0.0011050132\n",
      "323 0.0010594364\n",
      "324 0.001015322\n",
      "325 0.0009760099\n",
      "326 0.00093803107\n",
      "327 0.0009003957\n",
      "328 0.0008661575\n",
      "329 0.00083150365\n",
      "330 0.00080011826\n",
      "331 0.0007686375\n",
      "332 0.00074036146\n",
      "333 0.0007126442\n",
      "334 0.0006857435\n",
      "335 0.00066229515\n",
      "336 0.00063653087\n",
      "337 0.00061472127\n",
      "338 0.00059126137\n",
      "339 0.00056741154\n",
      "340 0.00055059674\n",
      "341 0.00052961556\n",
      "342 0.0005115403\n",
      "343 0.00049403653\n",
      "344 0.00047806028\n",
      "345 0.0004618161\n",
      "346 0.00044573433\n",
      "347 0.00043220943\n",
      "348 0.0004186274\n",
      "349 0.00040421938\n",
      "350 0.00039109544\n",
      "351 0.0003771728\n",
      "352 0.00036555255\n",
      "353 0.0003534736\n",
      "354 0.00034289743\n",
      "355 0.00033280283\n",
      "356 0.00032324524\n",
      "357 0.00031360183\n",
      "358 0.00030457764\n",
      "359 0.00029547932\n",
      "360 0.00028701455\n",
      "361 0.0002785183\n",
      "362 0.00027051813\n",
      "363 0.00026237543\n",
      "364 0.00025637384\n",
      "365 0.00024784473\n",
      "366 0.00024111479\n",
      "367 0.00023488115\n",
      "368 0.00022821943\n",
      "369 0.00022207125\n",
      "370 0.00021624332\n",
      "371 0.00021000816\n",
      "372 0.00020431705\n",
      "373 0.00019888554\n",
      "374 0.00019457671\n",
      "375 0.00018926196\n",
      "376 0.00018427112\n",
      "377 0.00017985795\n",
      "378 0.00017503739\n",
      "379 0.0001706615\n",
      "380 0.00016561823\n",
      "381 0.00016124824\n",
      "382 0.00015743502\n",
      "383 0.00015356386\n",
      "384 0.00015083235\n",
      "385 0.00014648032\n",
      "386 0.00014333162\n",
      "387 0.00013992065\n",
      "388 0.00013686786\n",
      "389 0.00013295504\n",
      "390 0.00013025745\n",
      "391 0.00012688311\n",
      "392 0.00012425659\n",
      "393 0.00012077042\n",
      "394 0.00011809198\n",
      "395 0.000115727365\n",
      "396 0.000112876674\n",
      "397 0.00011120533\n",
      "398 0.00010847387\n",
      "399 0.00010611993\n",
      "400 0.00010381909\n",
      "401 0.00010204434\n",
      "402 9.9329496e-05\n",
      "403 9.745428e-05\n",
      "404 9.565347e-05\n",
      "405 9.429176e-05\n",
      "406 9.189161e-05\n",
      "407 9.059656e-05\n",
      "408 8.8411296e-05\n",
      "409 8.6739485e-05\n",
      "410 8.518374e-05\n",
      "411 8.366718e-05\n",
      "412 8.1762846e-05\n",
      "413 8.030543e-05\n",
      "414 7.8868645e-05\n",
      "415 7.715187e-05\n",
      "416 7.5440206e-05\n",
      "417 7.43655e-05\n",
      "418 7.3207935e-05\n",
      "419 7.209359e-05\n",
      "420 7.079034e-05\n",
      "421 6.938189e-05\n",
      "422 6.814166e-05\n",
      "423 6.7242385e-05\n",
      "424 6.611131e-05\n",
      "425 6.497067e-05\n",
      "426 6.388073e-05\n",
      "427 6.2668696e-05\n",
      "428 6.1437e-05\n",
      "429 6.0361497e-05\n",
      "430 5.9135913e-05\n",
      "431 5.850271e-05\n",
      "432 5.776695e-05\n",
      "433 5.6350676e-05\n",
      "434 5.5732875e-05\n",
      "435 5.465324e-05\n",
      "436 5.3816882e-05\n",
      "437 5.2993848e-05\n",
      "438 5.253681e-05\n",
      "439 5.149252e-05\n",
      "440 5.0835028e-05\n",
      "441 4.9989678e-05\n",
      "442 4.9406146e-05\n",
      "443 4.8619488e-05\n",
      "444 4.776647e-05\n",
      "445 4.6974397e-05\n",
      "446 4.6559908e-05\n",
      "447 4.57006e-05\n",
      "448 4.4988115e-05\n",
      "449 4.42811e-05\n",
      "450 4.383116e-05\n",
      "451 4.3398104e-05\n",
      "452 4.2895124e-05\n",
      "453 4.216758e-05\n",
      "454 4.1324758e-05\n",
      "455 4.0698924e-05\n",
      "456 4.021397e-05\n",
      "457 3.9892402e-05\n",
      "458 3.9390186e-05\n",
      "459 3.8807848e-05\n",
      "460 3.8251364e-05\n",
      "461 3.7732338e-05\n",
      "462 3.715555e-05\n",
      "463 3.6640366e-05\n",
      "464 3.635818e-05\n",
      "465 3.590742e-05\n",
      "466 3.5746212e-05\n",
      "467 3.5317928e-05\n",
      "468 3.485907e-05\n",
      "469 3.4382312e-05\n",
      "470 3.391981e-05\n",
      "471 3.3666252e-05\n",
      "472 3.3063377e-05\n",
      "473 3.285975e-05\n",
      "474 3.2418648e-05\n",
      "475 3.2071748e-05\n",
      "476 3.1589745e-05\n",
      "477 3.1044863e-05\n",
      "478 3.0708958e-05\n",
      "479 3.0503492e-05\n",
      "480 3.0164478e-05\n",
      "481 2.976536e-05\n",
      "482 2.9511906e-05\n",
      "483 2.9085792e-05\n",
      "484 2.8874458e-05\n",
      "485 2.8710761e-05\n",
      "486 2.846869e-05\n",
      "487 2.8281374e-05\n",
      "488 2.7906208e-05\n",
      "489 2.763948e-05\n",
      "490 2.7379989e-05\n",
      "491 2.70306e-05\n",
      "492 2.6651585e-05\n",
      "493 2.6437585e-05\n",
      "494 2.6065785e-05\n",
      "495 2.5646506e-05\n",
      "496 2.553864e-05\n",
      "497 2.5409181e-05\n",
      "498 2.5166779e-05\n",
      "499 2.4818248e-05\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# First we set up the computational graph:\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create placeholders for the input and target data; these will be filled with real data when we execute the graph.\n",
    "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
    "\n",
    "# Create Variables for the weights and initialize them with random data.\n",
    "# A tensorflow Variable persists its value across executions of the graph.\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))\n",
    "\n",
    "# Forward pass: Compute the predicted y using operations on Tensorflow Tensors.\n",
    "# Note that this code does not actually perform any numeric operations; \n",
    "# it merely sets up the computational graph that we will execute later.\n",
    "h = tf.matmul(x, w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)\n",
    "\n",
    "# Compute loss using operations on Tensorflow Tensors\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2.0)\n",
    "\n",
    "# Compute gradient of the loss with respect to w1 and w2\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "\n",
    "# Update the weights using gradient descent. To actually update the weights we need to evaluate new_w1 and new_w2\n",
    "# when executing the graph. Note that in Tensorflow the act of updating the value of the weights is part of the \n",
    "# computational graph; in Pytorch this happens outside the computational graph.\n",
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "# Now we have built our computational graph, so we enter a Tensorflow session to actually execute the graph.\n",
    "\n",
    "# Create numpy arrays holding the actual data for the inputs x and targets y\n",
    "x_value = np.random.randn(N, D_in)\n",
    "y_value = np.random.randn(N, D_out)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the graph once to initialize the Variables w1 and w2.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "\n",
    "    for t in range(500):\n",
    "        # Execute the graph many times. Each time it executes we want to bind x_value and y_value to x and y\n",
    "        # specified with the feed_dict argument. \n",
    "        # Each time we execute the graph we want to compute the values for loss, new_w1 and new_w2; \n",
    "        # the values of these Tensors are returned as numpy arrays.\n",
    "        loss_value, _, _ = sess.run([loss, new_w1, new_w2], \n",
    "                                    feed_dict = {x: x_value, y: y_value})\n",
    "        \n",
    "        print(t, loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
